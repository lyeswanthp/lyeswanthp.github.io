<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Passage Ranking with Cross-Encoder | Lovely Yeswanth P</title>
    <link rel="stylesheet" href="../styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        .project-detail { max-width: 800px; margin: 100px auto 50px; padding: 0 1.5rem; }
        .back-link { display: inline-block; color: #6366f1; text-decoration: none; margin-bottom: 2rem; }
        .back-link:hover { text-decoration: underline; }
        .project-header { margin-bottom: 3rem; }
        .project-header h1 { font-size: 2.5rem; margin-bottom: 1rem; }
        .project-meta { display: flex; gap: 1rem; flex-wrap: wrap; margin-bottom: 2rem; }
        .project-meta a { color: #1a1a1a; text-decoration: none; padding: 0.5rem 1rem; background: #f5f5f5; border-radius: 6px; }
        .project-meta a:hover { background: #e5e5e5; }
        .section { margin-bottom: 2.5rem; }
        .section h2 { font-size: 1.5rem; margin-bottom: 1rem; }
        .section p, .section li { color: #666; line-height: 1.8; margin-bottom: 1rem; }
        ul { padding-left: 1.5rem; }
        .tech-list { display: flex; flex-wrap: wrap; gap: 0.5rem; margin-top: 1rem; }
        .tech-tag { padding: 0.5rem 1rem; background: #f5f5f5; border-radius: 6px; font-size: 0.9rem; }
        .metrics { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 1.5rem; margin: 2rem 0; }
        .metric-box { padding: 1.5rem; background: #f5f5f5; border-radius: 8px; text-align: center; }
        .metric-value { font-size: 2rem; font-weight: 700; color: #6366f1; }
        .metric-label { color: #666; margin-top: 0.5rem; }
    </style>
</head>
<body>
    <div class="project-detail">
        <a href="../index.html#projects" class="back-link">‚Üê Back to Projects</a>

        <div class="project-header">
            <h1>Passage Ranking Using Modified Cross-Encoder</h1>
        </div>

        <div class="section">
            <h2>Overview</h2>
            <p>
                A sophisticated passage ranking system that enhances information retrieval through a modified cross-encoder
                architecture. The model incorporates dual scoring mechanisms using both cosine similarity and logits-based
                relevance probabilities, achieving significant improvements in ranking accuracy and false negative reduction.
            </p>
        </div>

        <div class="metrics">
            <div class="metric-box">
                <div class="metric-value">21%</div>
                <div class="metric-label">False Negative Reduction</div>
            </div>
            <div class="metric-box">
                <div class="metric-value">9%</div>
                <div class="metric-label">Accuracy Increase</div>
            </div>
        </div>

        <div class="section">
            <h2>Key Features</h2>
            <ul>
                <li>Modified cross-encoder architecture with dual scoring mechanisms</li>
                <li>Combines cosine similarity and logits-based relevance probabilities</li>
                <li>Fine-tuned RoBERTa model for query encoding</li>
                <li>DistilBERT integration for efficient document encoding</li>
                <li>21% reduction in false negatives</li>
                <li>9% overall accuracy improvement</li>
            </ul>
        </div>

        <div class="section">
            <h2>Technical Approach</h2>
            <p>
                The system uses a novel dual-scoring approach that combines two complementary ranking signals. Cosine
                similarity provides a geometric measure of query-document alignment, while logits-based probabilities
                capture the model's confidence in relevance. This combination helps reduce both false positives and
                false negatives.
            </p>
            <p>
                RoBERTa, a robustly optimized BERT variant, handles query encoding with its enhanced pretraining approach.
                DistilBERT provides efficient document encoding, balancing performance with computational efficiency.
                The models are fine-tuned on domain-specific data to optimize ranking quality.
            </p>
        </div>

        <div class="section">
            <h2>Technologies Used</h2>
            <div class="tech-list">
                <span class="tech-tag">Python</span>
                <span class="tech-tag">RoBERTa</span>
                <span class="tech-tag">DistilBERT</span>
                <span class="tech-tag">Cross-Encoders</span>
                <span class="tech-tag">Information Retrieval</span>
                <span class="tech-tag">Re-ranking</span>
                <span class="tech-tag">Transformers</span>
            </div>
        </div>

        <div class="section">
            <h2>Applications</h2>
            <ul>
                <li>Search engines and document retrieval systems</li>
                <li>Question answering systems</li>
                <li>Recommendation engines</li>
                <li>Content discovery platforms</li>
                <li>Research paper search and citation analysis</li>
            </ul>
        </div>

        <div class="section">
            <h2>Impact</h2>
            <p>
                This modified cross-encoder approach demonstrates how combining multiple ranking signals can significantly
                improve retrieval quality. The 21% reduction in false negatives is particularly important for recall-critical
                applications, while the 9% overall accuracy improvement benefits precision-focused use cases. The dual-scoring
                mechanism provides a robust ranking framework applicable to various information retrieval tasks.
            </p>
        </div>
    </div>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Lovely Yeswanth Panchumarthi</p>
        </div>
    </footer>
</body>
</html>
